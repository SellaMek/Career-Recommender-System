# -*- coding: utf-8 -*-
"""CareerRecommender_RandomUser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nzhHLaoNdI-KaXFbynTPHiAelQco0i1i
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import random
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter

career_df = pd.read_csv('/content/drive/MyDrive/Datasets24/field_vs_occupation_dataset.csv')

# Ordinal encoding for Education Level
education_mapping = {
    'High School': 0,
    'Bachelor\'s': 1,
    'Master\'s': 2,
    'PhD': 3
}
career_df['Education Level Encoded'] = career_df['Education Level'].map(education_mapping)

# Replace original column with the encoded
career_df = career_df.drop('Education Level', axis=1)
career_df = career_df.rename(columns={'Education Level Encoded': 'Education Level'})

# Feature selection (no job security, skills gap, career events, or family influence)
numerical_features = [
    'Age', 'Years of Experience', 'Job Satisfaction',
    'Work-Life Balance', 'Salary', 'Job Opportunities',
    'Professional Networks', 'Technology Adoption', 'Education Level'
]

binary_features = [
    'Mentorship Available', 'Certifications', 'Freelancing Experience',
    'Geographic Mobility', 'Career Change Interest'
]

features = numerical_features + binary_features

# Scale data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(career_df[features])
scaled_df = pd.DataFrame(scaled_data, columns=features)

# KMeans -- collaborative filtering
kmeans = KMeans(n_clusters=5, random_state=42)
career_df['Cluster'] = kmeans.fit_predict(scaled_df)


# TF-IDF -- content-based filtering
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(career_df['Current Occupation'])

# Hybrid Recommender
def hybrid_recommender(user_input, df, scaled_df, scaler, kmeans_model, tfidf_matrix, alpha=0.5, top_n=3):
    # Convert input dict to vector (numerical + binary features)
    user_vector = np.array([user_input[feat] for feat in features]).reshape(1, -1)
    #user_scaled = scaler.transform(user_vector)
    user_scaled = scaler.transform(pd.DataFrame(user_vector, columns=features))


    # Content-Based: Cosine similarity (between user and current occupation)
    similarities = cosine_similarity(user_scaled, scaled_df).flatten()
    top_sim_indices = similarities.argsort()[::-1][:20]  # Get top 20 similar users
    content_careers = df.iloc[top_sim_indices]['Current Occupation']

    content_scores = Counter(content_careers)
    # Normalize content-based scores
    for k in content_scores:
        content_scores[k] /= len(top_sim_indices)

    # Collaborative: KMeans cluster matching (predict user's cluster based on their info)
    cluster_label = kmeans_model.predict(user_scaled)[0]
    cluster_peers = df[df['Cluster'] == cluster_label]['Current Occupation']

    collab_scores = Counter(cluster_peers)
    total = sum(collab_scores.values())
    for k in collab_scores:
        collab_scores[k] /= total

    # Combine content-based and collaborative scores
    hybrid_scores = {}
    for career in set(content_scores.keys()).union(collab_scores.keys()):
        hybrid_scores[career] = (
                alpha * content_scores.get(career, 0) +
                (1 - alpha) * collab_scores.get(career, 0)
        )

    # Return top 3 recommendation
    recommended = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]
    return [career for career, score in recommended]


# Random user input generation
def generate_random_user_input():
    # Simulating a random user input
    return {
        'Age': random.randint(22, 60),
        'Years of Experience': random.randint(0, 40),
        'Job Satisfaction': random.randint(1, 10),
        'Work-Life Balance': random.randint(1, 10),
        'Salary': random.randint(30000, 150000),
        'Job Opportunities': random.randint(50, 200),
        'Professional Networks': random.randint(1, 10),
        'Technology Adoption': random.randint(1, 10),
        'Education Level': random.choice([0, 1, 2, 3]),
        'Mentorship Available': random.choice([0, 1]),
        'Certifications': random.choice([0, 1]),
        'Freelancing Experience': random.choice([0, 1]),
        'Geographic Mobility': random.choice([0, 1]),
        'Career Change Interest': random.choice([0, 1])
    }

# Random actual occupations (ground truth)
def generate_random_actual_occupations():
    # Select random occupations for the "actual" occupations for this user
    return random.sample(career_df['Current Occupation'].unique().tolist(), 3)


# Top-N accuracy calculation
def calculate_top_n_accuracy(predictions, actual, top_n=3):
    relevant = set(actual)
    recommended = set(predictions[:top_n])  # Get the top N recommended occupations
    correct_recommendations = relevant.intersection(recommended)
    top_n_accuracy = len(correct_recommendations) / top_n
    return top_n_accuracy


# Generate random user input and actual occupations
random_user_input = generate_random_user_input()
actual_occupations = generate_random_actual_occupations()

# Get Top-N recommended occupations
hybrid_results = hybrid_recommender(
    user_input=random_user_input,
    df=career_df,
    scaled_df=scaled_df,
    scaler=scaler,
    kmeans_model=kmeans,
    tfidf_matrix=tfidf_matrix,
    alpha=0.2
)

# Calculate Top-N accuracy
top_n_accuracy = calculate_top_n_accuracy(hybrid_results, actual_occupations, top_n=3)

print(f"Random User Input: {random_user_input}")
print(f"Actual Occupations (Ground Truth): {actual_occupations}")
print(f"Top-N Recommended Occupations: {hybrid_results}")
print(f"Top-3 Accuracy: {top_n_accuracy:.4f}")

def evaluate_accuracy(recommended, actual_occupation, top_n=3):
    # Check how many of the top N recommended occupations match the actual occupation
    match_count = sum(1 for career in recommended if career == actual_occupation)
    return match_count / top_n  # Top-N accuracy

# Accumulate accuracies
accuracies = []

# Generate random users and evaluate accuracy 10 times
for i in range(10):
    random_user_input = generate_random_user_input()

    # Randomly pick an actual occupation from the dataset (or you can pick based on actual user data)
    actual_occupation = random.choice(career_df['Current Occupation'].values)

    # Get hybrid recommendations for the random user
    recommended_occupations = hybrid_recommender(
        user_input=random_user_input,
        df=career_df,
        scaled_df=scaled_df,
        scaler=scaler,
        kmeans_model=kmeans,
        tfidf_matrix=tfidf_matrix,
        alpha=0.3
    )

    # Evaluate the accuracy of the recommendation
    accuracy = evaluate_accuracy(recommended_occupations, actual_occupation, top_n=3)

    # Print the results for each random user
    print(f"Random User {i+1}:")
    print(f"Actual Occupation: {actual_occupation}")
    print(f"Recommended Occupations: {recommended_occupations}")
    print(f"Top-N Accuracy: {accuracy*100:.2f}%\n")

    # Add the accuracy to the list
    accuracies.append(accuracy)

# Calculate and print the average accuracy
average_accuracy = sum(accuracies) / len(accuracies)
print(f"Average Top-N Accuracy across 10 Random Users: {average_accuracy*100:.2f}%")

# distribution of clusters
plt.figure(figsize=(8, 6))
sns.countplot(x='Cluster', data=career_df)
plt.title('Distribution of Clusters')
plt.xlabel('Cluster')
plt.ylabel('Number of Users')
plt.show()

# visualization for tf-idf

# Feature Importance from TF-IDF
feature_names = tfidf.get_feature_names_out()
tfidf_sums = tfidf_matrix.sum(axis=0)
tfidf_scores = np.array(tfidf_sums)[0]

importance_df = pd.DataFrame({'Feature': feature_names, 'TF-IDF Score': tfidf_scores})
importance_df = importance_df.sort_values(by='TF-IDF Score', ascending=False).head(10)


plt.figure(figsize=(12,6))
sns.barplot(x='TF-IDF Score', y='Feature', data=importance_df, palette="Blues_d")
plt.title('Top 10 Important Words from TF-IDF')
plt.xlabel('TF-IDF Score')
plt.ylabel('Word')
plt.show()

# Interactive visualization
import plotly.express as px

fig = px.scatter(career_df, x='Salary', y='Job Satisfaction', color='Cluster', hover_data=['Current Occupation', 'Age', 'Years of Experience'])
fig.update_layout(title='Interactive Scatter Plot of Salary vs. Job Satisfaction')
fig.show()